# 高阶函数

- 高阶函数主要有两种，一种是将一个函数当做另一个函数的参数；另一种是函数的返回值是函数，即高阶函数可以产生新的函数.

## Scala 中常用高阶函数

### 1. map 函数

- 所有集合类型都有 map 函数

代码：

```scala
package higher_order_function

/**
  * map 函数, 将一个函数传入map中，然后利用传入的这个函数，将集合中的每个元素处理，并将处理后的结果返回
  */
object MapFun {

  def main(args: Array[String]): Unit = {
    // Array 类型
    val array1 = Array("spark", "hive", "hadoop")
    print("array1 = ")
    for(i <- array1) {
      print(i + " ")
    }

    val array2 = array1.map((x: String) => x * 2)
    print("\narray2 = ")
    for(i <- array2) {
      print(i + " ")
    }

    // List 类型
    val list1 = List("spark" -> 1, "hive" -> 2, "hadoop" -> 3)
    println("\nlist1 = " + list1)

    val list2 = list1.map(x => x._1)
    println("list2 = " + list2)

    // Map 类型
    val map1 = Map("spark" -> 1, "hive" -> 2, "hadoop" -> 3)
    println("map1 = " + map1)

    val map2 = map1.map(_._2)
    println("map2 = " + map2)

  }

}
```

运行结果：

	array1 = spark hive hadoop 
	array2 = sparkspark hivehive hadoophadoop 
	list1 = List((spark,1), (hive,2), (hadoop,3))
	list2 = List(spark, hive, hadoop)
	map1 = Map(spark -> 1, hive -> 2, hadoop -> 3)
	map2 = List(1, 2, 3)
	
	
### 2. flatMap 函数

代码：

```scala
package higher_order_function

/**
  * flatMap 函数, flatMap 与 map 唯一不一样的地方就是传入的函数在处理完后返回值必须是 List
  */
object FlatMapFun {
  def main(args: Array[String]): Unit = {
    val list1 = List(List(1, 2, 3), List(2, 3, 4))
    println("list1 = " + list1)
    val list2 = list1.flatMap(x => x)
    println("list2 = " + list2)
  }

}
```

运行结果：

	list1 = List(List(1, 2, 3), List(2, 3, 4))
	list2 = List(1, 2, 3, 2, 3, 4)
	
### 3. filter 函数

代码：

```scala
package higher_order_function

/**
  * filter 函数， 过滤
  */
object FilterFun {
  def main(args: Array[String]): Unit = {
    val array1 = Array(1, 2, 3, 4, 5)
    print("array1 = ")
    for(i <- array1) {
      print(i + " ")
    }

    val array2 = array1.filter(_>3)
    print("\narray2 = ")
    for(i <- array2) {
      print(i + " ")
    }
  }

}
```

运行结果：

	array1 = 1 2 3 4 5 
	array2 = 4 5 
	
### 4. fold 函数

- fold 函数需要两个参数，一个初始值和一个函数。输入函数也需要两个参数：累加值和当前的索引

- fold遍历的顺序没有特殊的次序，foldLeft是从左开始计算，然后往右遍历；foldRight是从右开始算，然后往左遍历
	
### 5. reduce 函数

- 使用 reduce 可以处理集合中的每个元素并返回一个值，reduce 不保证处理方向

- reduceLeft 和 reduceRight 制定了处理方向

- reduce 是 fold 一种特殊情况,但是 reduce 返回的类型必须是集合元素的类型或其父类

- 

代码：

```scala
package higher_order_function

/**
  * reduce 函数，使用reduce可以处理集合的每个元素并返回一个值,reduce 不保证方向
  * 通过使用 reduceLeft 和 reduceRight 可以强制处理元素的方向
  */
object ReduceFun {
  def main(args: Array[String]): Unit = {
    // 方法一
    val array = Array(1, 2, 3, 4, 5)
    print("array = ")
    for(i <- array) print(i + " ")

    val sum1 = array.reduce(_+_)
    println("\nsum1 = " + sum1)

    // 方法二
    val sum2 = array.reduce((x: Int, y: Int) => { println(x, y); x + y })
    println("reduce sum2 = " + sum2)

    val sum3 = array.reduceLeft((x: Int, y: Int) => { println(x, y); x + y })
    println("reduceLeft sum3 = " + sum3)

    val sum4 = array.reduceRight((x: Int, y: Int) => { println(x, y); x + y })
    println("reduceRight sum4 = " + sum4)

    // List
    val list = List("Spark", "Hive", "Hadoop")
    println("list1 = " + list)

    val str = list.reduce(_+_)
    println("str = " + str)
  }

}
```

运行结果：

	array = 1 2 3 4 5 
	sum1 = 15
	(1,2)
	(3,3)
	(6,4)
	(10,5)
	reduce sum2 = 15
	(1,2)
	(3,3)
	(6,4)
	(10,5)
	reduceLeft sum3 = 15
	(4,5)
	(3,9)
	(2,12)
	(1,14)
	reduceRight sum4 = 15
	list1 = List(Spark, Hive, Hadoop)
	str = SparkHiveHadoop